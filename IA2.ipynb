{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 19:42:15.305196: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 19:42:15.936068: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nickotronz7/miniconda3/envs/tf/lib/:/home/nickotronz7/miniconda3/envs/tf/lib/\n",
      "2023-04-04 19:42:15.938487: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/nickotronz7/miniconda3/envs/tf/lib/:/home/nickotronz7/miniconda3/envs/tf/lib/\n",
      "2023-04-04 19:42:15.938497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D\n",
    "from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Concatenate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (256, 256, 3)\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "KernelSize = (3, 3)\n",
    "#     0   1   2   3   4\n",
    "f = [16, 32, 64, 128, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/mnt/s/Proyects/dataset/train/outdoor\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"/mnt/s/Proyects/dataset/val\",\n",
    "    batch_size=32,\n",
    "    image_size=(256, 256),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(HEIGHT, WIDTH, CHANNELS))\n",
    "\n",
    "x = inputs\n",
    "\n",
    "# Encoder\n",
    "# f[0]\n",
    "d = Conv2D(f[0], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(d)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(f[0], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x += d\n",
    "\n",
    "p1 = MaxPool2D((2, 2), (2, 2))(x)\n",
    "c1 = x\n",
    "\n",
    "# f[1]\n",
    "d = Conv2D(f[1], KernelSize, padding='same', strides=1, activation='relu')(p1)\n",
    "x = BatchNormalization()(d)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(f[1], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x += d\n",
    "\n",
    "p2 = MaxPool2D((2, 2), (2, 2))(x)\n",
    "c2 = x\n",
    "\n",
    "# f[2]\n",
    "d = Conv2D(f[2], KernelSize, padding='same', strides=1, activation='relu')(p2)\n",
    "x = BatchNormalization()(d)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(f[2], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x += d\n",
    "\n",
    "p3 = MaxPool2D((2, 2), (2, 2))(x)\n",
    "c3 = x\n",
    "\n",
    "# Bottelneck\n",
    "x = Conv2D(f[3], KernelSize, padding='same', strides=1, activation='relu')(p3)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "x = Conv2D(f[3], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# Decoder\n",
    "# f[2]\n",
    "x = UpSampling2D((2,2))(x)\n",
    "concat = Concatenate()([x, c3])\n",
    "x = Conv2D(f[2], KernelSize, padding='same', strides=1, activation='relu')(concat)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(f[2], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# f[1]\n",
    "x = UpSampling2D((2,2))(x)\n",
    "concat = Concatenate()([x, c2])\n",
    "x = Conv2D(f[1], KernelSize, padding='same', strides=1, activation='relu')(concat)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(f[1], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "# f[0]\n",
    "x = UpSampling2D((2,2))(x)\n",
    "concat = Concatenate()([x, c1])\n",
    "x = Conv2D(f[0], KernelSize, padding='same', strides=1, activation='relu')(concat)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(f[0], KernelSize, padding='same', strides=1, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "x = Conv2D(1,(1,1), padding='same', activation=\"tanh\", name=\"outputs\")(x)\n",
    "\n",
    "outputs = x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, epochs=10, validation_data=val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, batch_size=6, dim=(768, 1024), n_channels=3, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialization\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.indices = self.data.index.tolist()\n",
    "        self.dim = dim\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.min_depth = 0.1\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (index + 1) * self.batch_size > len(self.indices):\n",
    "            self.batch_size = len(self.indices) - index * self.batch_size\n",
    "        # Generate one batch of data\n",
    "        # Generate indices of the batch\n",
    "        index = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        # Find list of IDs\n",
    "        batch = [self.indices[k] for k in index]\n",
    "        x, y = self.data_generation(batch)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        self.index = np.arange(len(self.indices))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.index)\n",
    "\n",
    "    def load(self, image_path, depth_map, mask):\n",
    "        \"\"\"Load input and target image.\"\"\"\n",
    "\n",
    "        image_ = cv2.imread(image_path)\n",
    "        image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n",
    "        image_ = cv2.resize(image_, self.dim)\n",
    "        image_ = tf.image.convert_image_dtype(image_, tf.float32)\n",
    "\n",
    "        depth_map = np.load(depth_map).squeeze()\n",
    "\n",
    "        mask = np.load(mask)\n",
    "        mask = mask > 0\n",
    "\n",
    "        max_depth = min(300, np.percentile(depth_map, 99))\n",
    "        depth_map = np.clip(depth_map, self.min_depth, max_depth)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "        depth_map = np.log(depth_map, where=mask)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "        depth_map = np.ma.masked_where(~mask, depth_map)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "        depth_map = np.clip(depth_map, 0.1, np.log(max_depth, where=depth_map>0))\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "        depth_map = cv2.resize(depth_map, self.dim)\n",
    "        depth_map = np.expand_dims(depth_map, axis=2)\n",
    "        depth_map = tf.image.convert_image_dtype(depth_map, tf.float32)\n",
    "\n",
    "        return image_, depth_map\n",
    "\n",
    "    def data_generation(self, batch):\n",
    "\n",
    "        x = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.dim, 1))\n",
    "\n",
    "        for i, batch_id in enumerate(batch):\n",
    "            x[i,], y[i,] = self.load(\n",
    "                self.data[\"image\"][batch_id],\n",
    "                self.data[\"depth\"][batch_id],\n",
    "                self.data[\"mask\"][batch_id],\n",
    "            )\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_depth_map(samples, test=False, model=None):\n",
    "    input, target = samples\n",
    "    cmap = plt.cm.jet\n",
    "    cmap.set_bad(color=\"black\")\n",
    "\n",
    "    if test:\n",
    "        pred = model.predict(input)\n",
    "        fig, ax = plt.subplots(6, 3, figsize=(50, 50))\n",
    "        for i in range(6):\n",
    "            ax[i, 0].imshow((input[i].squeeze()))\n",
    "            ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)\n",
    "            ax[i, 2].imshow((pred[i].squeeze()), cmap=cmap)\n",
    "        fig.savefig(\"predictions\"+str(datetime.datetime.today().time()).replace(':','')+\".png\")\n",
    "\n",
    "    else:\n",
    "        fig, ax = plt.subplots(6, 2, figsize=(50, 50))\n",
    "        for i in range(6):\n",
    "            ax[i, 0].imshow((input[i].squeeze()))\n",
    "            ax[i, 1].imshow((target[i].squeeze()), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/mnt/s/Proyects/dataset/val\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "filelist = []\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        filelist.append(os.path.join(root, file))\n",
    "\n",
    "filelist.sort()\n",
    "data = {\n",
    "    \"image\": [x for x in filelist if x.endswith(\".png\")],\n",
    "    \"depth\": [x for x in filelist if x.endswith(\"_depth.npy\")],\n",
    "    \"mask\": [x for x in filelist if x.endswith(\"_depth_mask.npy\")],\n",
    "}\n",
    "val_df = pd.DataFrame(data)\n",
    "\n",
    "val_df = val_df.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = next(\n",
    "    iter(\n",
    "        DataGenerator(\n",
    "            data=val_df[265:].reset_index(drop=\"true\"), batch_size=6, dim=(HEIGHT, WIDTH)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "visualize_depth_map(test_loader, test=True, model=model)\n",
    "\n",
    "test_loader = next(\n",
    "    iter(\n",
    "        DataGenerator(\n",
    "            data=val_df[300:].reset_index(drop=\"true\"), batch_size=6, dim=(HEIGHT, WIDTH)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "visualize_depth_map(test_loader, test=True, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f4ccfa8962978aab316b287c2da7261793f3a2aa1970ad2942cb591025406d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
